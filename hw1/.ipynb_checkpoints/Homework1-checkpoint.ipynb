{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822cd234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffy011/miniconda3/envs/msse-python/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy.optimize import minimize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403ce291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print('func:%r took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85f4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def draw_path(func,path,x_min=-2,x_max=2,y_min=-2,y_max=2):\n",
    "    a=np.linspace(x_min,x_max,100)\n",
    "    b=np.linspace(y_min,y_max,100)\n",
    "    x,y=np.meshgrid(a,b)\n",
    "    z=func((x,y))\n",
    "    fig,ax=plt.subplots()\n",
    "    my_contour=ax.contour(x,y,z,50)\n",
    "    plt.colorbar(my_contour)\n",
    "    ax.plot(path[:,0],path[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71b2c6",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "\n",
    "### 1. Bisection vs. Golden Section. In class we used the simple bisection method  to  take  the  first  step  in  isolating  at  least  one  minimum  for  the function  shown.  This  first  step  in  placement  of  d  reduced  the  original interval [a,b,c] =1.0 to [a,b,d] = 0.75.  But in general, the average size interval  [L]  after  Step  1  is  determined  by  the  equal  probability  of placing point d in  either sub-interval, such that [L1] =P(left-interval) x  Â½ + P(right-interval) x Â¾ = 0.625 (since you canâ€™t a priori know the best half) \n",
    "\n",
    "#### (a) For step 2, place point e at the bisector of larger interval [a,b]. Why is this better than [b,d]? \n",
    "##### It is better to place the bisector a the larger interval as there is a higher probability of finding the minima at the large search space than it is in smaller space, which in this case is [a,b] and the smaller space is [b,d]. \n",
    "\n",
    "#### (b) What is the new interval and how much is the search space reduced? \n",
    "##### Based on the image provided, the new interval is [a,e,b] and the search space is reduced by 1/3. The total new search space is 2/3. \n",
    "\n",
    "#### (c) For step 3, reduce the size of the interval from step 2 by placing point f at the bisection of your choice. \n",
    "##### I chose to place point f in the bisection of [a,e], creating a new search space of triplets [f,e,b]. This reduces the search space by 1/4 and the new search space is 3/4 of [a,e,b]. \n",
    "\n",
    "#### (d) fill in tree for all possible size intervals for steps 2 and 3. Write your answers in ratios to the interval size of the previous step. \n",
    "##### \n",
    "\n",
    "#### (e) What is average size of interval at steps 2 and 3? \n",
    "#####\n",
    "\n",
    "#### (f)  How  much  does  Golden  Section  improve  over  Bisection  at  each  step?  Please  use  a  chart  of steps v.s. different methods to show their difference. \n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acee23b",
   "metadata": {},
   "source": [
    "# Question 2 \n",
    "\n",
    "### 2. Local optimization using 1st and quasi-2nd order methods. You will solve the following optimization problem using  a  python  code  you  develop  for  the  steepest  descents method! For the function  ð‘“(ð‘¥,ð‘¦)=ð‘¥4 âˆ’ð‘¥2 +ð‘¦2 +2ð‘¥ð‘¦âˆ’2 there  are  three  stationary  points  found  over  the  range  x  =  [-2,2] and y = [-2,2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10297216",
   "metadata": {},
   "source": [
    "#### (a)  Starting  from  point  (1.5,1.5),  and  with  stepsize  =0.1, determine  new (ð‘¥,ð‘¦) position  using  one  step  of  the  steepest descent algorithm (check against the debugging output). Is it a good optimization step? Depending on this outcome, how will you change the stepsize in the next step? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6f90f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new x and y values are: (0.1499999999999999, 0.8999999999999999)\n",
      "The function evaluation with x0 and y0 is: 7.5625\n",
      "The function evaluation with x1 and y1 is: -0.9419937500000004\n"
     ]
    }
   ],
   "source": [
    "# The function is f(x,y) = x^4 - x^2 + +y^2 +2xy -2  \n",
    "\n",
    "def f(starting_point):\n",
    "    x = starting_point[0]\n",
    "    y = starting_point[1]\n",
    "    return x**4 - x**2 + y**2 + 2*x*y - 2\n",
    "\n",
    "def dfdx(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return 4*x**3 - 2*x + 2*y\n",
    "\n",
    "def dfdy(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return 2*y + 2*x\n",
    "\n",
    "def steepest_descent(x,y,stepsize):\n",
    "    x_new = x - stepsize*dfdx([x,y])\n",
    "    y_new = y - stepsize*dfdy([x,y])\n",
    "    return x_new, y_new\n",
    "\n",
    "x = 1.5 \n",
    "y = 1.5 \n",
    "stepsize = 0.1 \n",
    "x_new, y_new = steepest_descent(x,y,stepsize)\n",
    "print(f'The new x and y values are: {steepest_descent(x,y,stepsize)}')\n",
    "print(f'The function evaluation with x0 and y0 is: {f([x,y])}')\n",
    "print(f'The function evaluation with x1 and y1 is: {f([x_new, y_new])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb89d5",
   "metadata": {},
   "source": [
    "##### The new function evaluation is lower than the initial value, thus it can be established that this is a good step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f388e03",
   "metadata": {},
   "source": [
    "#### b) Implement  the  steepest  decent  using  the  provided  template.  Continue  executing  steepest descents. How many steps does it take to converge to the local minimum to tolerance = 1 x 10-5 of the gradient (check against the debugging output and compare code timings)? Note: You donâ€™t need to use line search, just take one step in the search direction, and use the following stepsize update: ðœ†={1.2 ðœ† ð‘“ð‘œð‘Ÿ ð‘Ž ð‘”ð‘œð‘œð‘‘ ð‘ ð‘¡ð‘’ð‘ and 0.5 ðœ† ð‘“ð‘œð‘Ÿ ð‘Ž ð‘ð‘Žð‘‘ ð‘ ð‘¡ð‘’ð‘ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29260a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "func:'steepest_descent' took: 0.0040 sec\n",
      "{'x': array([-0.99999892,  0.99999943]), 'evaluation': -2.999999999995068, 'path': array([[ 1.5       ,  1.5       ],\n",
      "       [ 0.15      ,  0.9       ],\n",
      "       [-0.03162   ,  0.648     ],\n",
      "       [-0.22733235,  0.47048256],\n",
      "       [-0.4603766 ,  0.38644985],\n",
      "       [-0.73063964,  0.41710875],\n",
      "       [-0.91361447,  0.57314178],\n",
      "       [-0.89067253,  0.77647099],\n",
      "       [-1.072703  ,  0.85831194],\n",
      "       [-0.88004038,  0.93513214],\n",
      "       [-1.07440969,  0.91144369],\n",
      "       [-0.8191814 ,  0.99553056],\n",
      "       [-1.00371455,  0.95003442],\n",
      "       [-0.98247032,  0.96665308],\n",
      "       [-1.00196259,  0.97252925],\n",
      "       [-0.98533102,  0.98565078],\n",
      "       [-1.0162044 ,  0.98547972],\n",
      "       [-0.99022477,  0.99369805],\n",
      "       [-1.00370679,  0.9925832 ],\n",
      "       [-0.9936794 ,  0.99686773],\n",
      "       [-1.00672832,  0.99539405],\n",
      "       [-0.99782619,  0.99801346],\n",
      "       [-1.00028169,  0.99796153],\n",
      "       [-0.99913443,  0.99873366],\n",
      "       [-1.00035525,  0.9988937 ],\n",
      "       [-0.99897351,  0.99959411],\n",
      "       [-1.00010453,  0.99944541],\n",
      "       [-0.99979477,  0.99963492],\n",
      "       [-1.00002278,  0.99969008],\n",
      "       [-0.9998473 ,  0.99982783],\n",
      "       [-1.00014104,  0.9998375 ],\n",
      "       [-0.99992545,  0.99991291],\n",
      "       [-1.0000106 ,  0.99991665],\n",
      "       [-0.99996182,  0.99995026],\n",
      "       [-1.00002241,  0.99995522],\n",
      "       [-0.99998875,  0.99996964],\n",
      "       [-0.99999542,  0.99997456],\n",
      "       [-0.99999464,  0.99998101],\n",
      "       [-0.99999754,  0.99998606],\n",
      "       [-0.99999681,  0.99999117],\n",
      "       [-1.00000061,  0.99999418],\n",
      "       [-0.99999493,  0.9999983 ],\n",
      "       [-1.00000251,  0.99999722],\n",
      "       [-0.99999661,  0.99999926],\n",
      "       [-1.00000408,  0.99999804],\n",
      "       [-0.99999892,  0.99999943]])}\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "import numpy.linalg as LA\n",
    "\n",
    "def f(starting_point):\n",
    "    x = starting_point[0]\n",
    "    y = starting_point[1]\n",
    "    return x**4 - x**2 + y**2 + 2*x*y - 2\n",
    "\n",
    "def dfdx(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return 4*x**3 - 2*x + 2*y\n",
    "\n",
    "def dfdy(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return 2*y + 2*x\n",
    "\n",
    "@timeit\n",
    "def steepest_descent(func,first_derivate,starting_point,stepsize,tol):\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "    deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "    deriv = np.array([deriv_x,deriv_y])\n",
    "    # initialize the counter\n",
    "    count=0\n",
    "    visited=[]\n",
    "    visited.append(starting_point)\n",
    "    while LA.norm(deriv) > tol and count < 1e6:\n",
    "        # calculate new point position\n",
    "        new_point_x = starting_point[0] - stepsize*deriv[0]\n",
    "        new_point_y = starting_point[1] - stepsize*deriv[1]\n",
    "        if func([new_point_x, new_point_y]) < func([starting_point[0], starting_point[1]]):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            stepsize = stepsize*1.2\n",
    "            visited.append(np.array([new_point_x,new_point_y]))\n",
    "            starting_point = np.array([new_point_x,new_point_y])\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            stepsize = stepsize*0.5\n",
    "            visited.append(np.array([new_point_x,new_point_y]))\n",
    "            starting_point = np.array([new_point_x,new_point_y])\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "        count+=1\n",
    "    # return the results\n",
    "    return {\"count\":count, \"x\":starting_point,\"evaluation\":func([starting_point[0], starting_point[1]]),\"path\":np.asarray(visited)}\n",
    "\n",
    "# define the starting point\n",
    "starting_point = np.array([1.5,1.5])\n",
    "# define the stepsize\n",
    "stepsize = 0.1\n",
    "# define the tolerance\n",
    "tol = 1e-5\n",
    "# call the function\n",
    "# set the derivative \n",
    "deriv = np.array([dfdx, dfdy])\n",
    "result = steepest_descent(f, deriv,starting_point,stepsize,tol)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cbf5b",
   "metadata": {},
   "source": [
    "#### (c) Compare your steepest descent code against conjugate gradients (CG), and BFGS to determine the local minimum starting from (1.5,1.5). In terms of number of steps, are conjugate gradients and/or BFGS more efficient than steepest descents? Note: See SciPy documentation on how to use CG  and BFGS, examples available  at the end of webpage: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79748049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 26\n",
      "     fun: -2.9999999999995843\n",
      "     jac: array([ 2.08616257e-07, -1.10268593e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 78\n",
      "     nit: 9\n",
      "    njev: 26\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99999984,  0.99999928])\n"
     ]
    }
   ],
   "source": [
    "#CG methods \n",
    "res = minimize(f, starting_point, method='CG', options={'disp':True, 'gtol':1e-5})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deadfdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n",
      "      fun: -2.9999999999998184\n",
      " hess_inv: array([[ 0.12457747, -0.12457762],\n",
      "       [-0.12457762,  0.62569996]])\n",
      "      jac: array([-1.63912773e-06, -2.98023224e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 24\n",
      "      nit: 7\n",
      "     njev: 8\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.99999979, -0.99999979])\n"
     ]
    }
   ],
   "source": [
    "#BFGS methods \n",
    "res = minimize(f, starting_point, method='BFGS', options={'disp':True, 'gtol':1e-5})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8275b50",
   "metadata": {},
   "source": [
    "# Question 3 \n",
    "\n",
    "### 3. Local optimization and machine learning using Stochastic Gradient Descent (SGD). The Rosenbrock Banana Function looks innocuous enough ð‘“(ð‘¥,ð‘¦)=(1âˆ’ð‘¥)2 +10(ð‘¦âˆ’ð‘¥2)2 with only one (global) minimum at (ð‘¥,ð‘¦)=(1.0,1.0)! ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eafc57",
   "metadata": {},
   "source": [
    "#### (a)  Starting  at  ð‘¥ =âˆ’0.5 and  ð‘¦=1.5,  and  using  your  code  for  steepest descents with stepsize =0.1, how many steps to converge to the minimum? Use a tolerance = 1 x 10-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94520c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "func:'steepest_descent' took: 0.0030 sec\n",
      "{'x': array([nan, inf]), 'evaluation': nan, 'path': array([[-5.00000000e-001,  1.50000000e+000],\n",
      "       [-2.70000000e+000, -1.00000000e+000],\n",
      "       [ 4.24360000e+001,  7.29000000e+000],\n",
      "       [-7.60696243e+004,  9.04052048e+002],\n",
      "       [ 2.20091744e+014,  1.44664761e+009],\n",
      "       [-2.66533168e+042,  6.05504695e+027],\n",
      "       [ 2.36681219e+126,  4.43999561e+083],\n",
      "       [            -inf,  1.75056248e+251],\n",
      "       [             nan,              inf]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182/1898835366.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (1-x)**2 + 10*(y-x**2)**2\n",
      "/tmp/ipykernel_182/1898835366.py:9: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return -2*(1-x) - 40*x*(y-x**2)\n",
      "/tmp/ipykernel_182/2874953301.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_point_x = starting_point[0] - stepsize*deriv[0]\n"
     ]
    }
   ],
   "source": [
    "def f(starting_point):\n",
    "    x = starting_point[0]\n",
    "    y = starting_point[1]\n",
    "    return (1-x)**2 + 10*(y-x**2)**2\n",
    "\n",
    "def dfdx(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return -2*(1-x) - 40*x*(y-x**2)\n",
    "\n",
    "def dfdy(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return 20*(y-x**2)\n",
    "\n",
    "# define the starting point\n",
    "starting_point = np.array([-0.5,1.5])\n",
    "# define the stepsize\n",
    "stepsize = 0.1\n",
    "# define the tolerance\n",
    "tol = 1e-5\n",
    "# call the function\n",
    "# set the derivative \n",
    "deriv = np.array([dfdx, dfdy])\n",
    "result = steepest_descent(f, deriv,starting_point,stepsize,tol)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d647cb",
   "metadata": {},
   "source": [
    "#### (b) By adding a small amount of stochastic noise to the gradient at every step (In your code add a random vector that is the same norm as the gradient at that step), which is equivalent to a small batch  derivative  of  any  loss  function  in  deep  learning,  implement  your  own  stochastic  gradient descent  code  by  modifying  on  your  steepest  descent  code,  and  run  the  SGD  algorithm.  (Check against debugging outputs.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b664e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'stochastic_gradient_descent' took: 0.0154 sec\n",
      "{'count': 69, 'x': array([-2.85363700e-06,  2.46252421e-06]), 'evaluation': 1.532336355154215e-11, 'path': array([[-5.00000000e-01,  1.50000000e+00],\n",
      "       [-4.83786867e-01,  1.45591700e+00],\n",
      "       [-4.85847029e-01,  1.39774246e+00],\n",
      "       [-4.57283503e-01,  1.38359312e+00],\n",
      "       [-4.25211679e-01,  1.36888651e+00],\n",
      "       [-4.20257509e-01,  1.36867935e+00],\n",
      "       [-3.65786417e-01,  1.29116286e+00],\n",
      "       [-3.17979037e-01,  1.26989747e+00],\n",
      "       [-3.49561551e-01,  1.11522405e+00],\n",
      "       [-3.05472729e-01,  1.10512764e+00],\n",
      "       [-3.61588863e-01,  9.26147044e-01],\n",
      "       [-2.64479008e-01,  8.88904566e-01],\n",
      "       [-2.10534709e-01,  6.74113028e-01],\n",
      "       [-2.18746716e-01,  6.72839720e-01],\n",
      "       [-2.28340565e-01,  4.33436679e-01],\n",
      "       [-1.11009074e-01,  2.74472543e-01],\n",
      "       [-2.21563157e-02,  1.73184821e-01],\n",
      "       [-5.58135581e-02,  1.72514891e-01],\n",
      "       [-8.13237395e-02,  5.44936602e-02],\n",
      "       [ 5.97428288e-02,  6.51792477e-02],\n",
      "       [-2.79522201e-02,  3.43807833e-02],\n",
      "       [-2.86800096e-02,  3.27592833e-02],\n",
      "       [-2.00288746e-02,  9.99655292e-03],\n",
      "       [ 2.41272765e-03, -4.82607361e-03],\n",
      "       [ 2.77021342e-03, -1.12095396e-03],\n",
      "       [-3.80770011e-03, -1.70028911e-03],\n",
      "       [-3.44977004e-03, -2.32427104e-03],\n",
      "       [-3.70692562e-03,  2.21285667e-04],\n",
      "       [ 2.01932166e-03, -1.77693465e-03],\n",
      "       [-1.59882414e-03, -9.51113300e-04],\n",
      "       [ 1.87060830e-03, -2.37841152e-03],\n",
      "       [ 1.99930415e-03, -1.82387117e-03],\n",
      "       [ 1.93076257e-03, -9.50296234e-04],\n",
      "       [-6.06207250e-04,  4.77868870e-04],\n",
      "       [-5.80927835e-04,  1.75850062e-04],\n",
      "       [ 3.33005187e-04,  1.01198814e-03],\n",
      "       [-5.52496451e-04,  9.54339755e-04],\n",
      "       [-6.30555723e-04,  8.54270945e-04],\n",
      "       [-6.05876660e-04,  2.66669862e-04],\n",
      "       [ 4.03705481e-04, -2.77596390e-04],\n",
      "       [ 2.90683455e-04,  9.76121947e-05],\n",
      "       [-6.79795641e-04,  3.16186862e-04],\n",
      "       [-6.13430479e-04,  6.90988008e-05],\n",
      "       [-4.63836677e-04,  6.23450011e-04],\n",
      "       [-4.90045272e-04,  5.76415615e-04],\n",
      "       [-4.17029527e-04,  2.75603461e-04],\n",
      "       [-3.31216762e-05,  4.90534342e-04],\n",
      "       [-1.29283711e-04,  5.05127934e-04],\n",
      "       [-2.62834947e-04,  4.62903169e-04],\n",
      "       [-5.96108414e-05,  3.70902622e-04],\n",
      "       [-1.96001599e-04,  2.67898039e-04],\n",
      "       [-1.54129981e-04,  9.38543095e-05],\n",
      "       [-1.54318699e-04,  8.99720290e-05],\n",
      "       [-1.23663284e-04,  1.74034708e-04],\n",
      "       [-1.37487140e-04,  4.52313712e-05],\n",
      "       [ 1.24320771e-04,  2.65989153e-04],\n",
      "       [ 1.05050301e-04,  2.85600995e-04],\n",
      "       [-3.48104698e-05, -1.20187494e-04],\n",
      "       [ 1.29422177e-05, -1.46857290e-04],\n",
      "       [ 1.18778280e-04,  2.98685346e-05],\n",
      "       [-6.49993752e-05,  1.30781870e-05],\n",
      "       [-5.10640704e-05, -1.52739933e-05],\n",
      "       [ 1.42347502e-05,  6.48641848e-05],\n",
      "       [-7.13657506e-05,  5.27785641e-05],\n",
      "       [ 1.96356464e-07,  4.29737524e-05],\n",
      "       [-1.67676592e-05,  1.16769345e-05],\n",
      "       [-2.88170924e-06, -2.00664197e-06],\n",
      "       [-3.28010538e-06,  3.95411817e-07],\n",
      "       [ 1.96291091e-06,  5.14164510e-06],\n",
      "       [-2.85363700e-06,  2.46252421e-06]])}\n"
     ]
    }
   ],
   "source": [
    "@timeit\n",
    "def stochastic_gradient_descent(func,first_derivate,starting_point,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    '''stochastic_injection: controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "        0 for no stochasticity, equivalent to SD. \n",
    "        Use 1 in this homework to run SGD\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "    deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "    deriv = np.array([deriv_x,deriv_y])\n",
    "    count=0\n",
    "    visited=[]\n",
    "    visited.append(starting_point)\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            # but has a random direction\n",
    "            stochastic_deriv=np.random.normal(0,1,2)\n",
    "            stochastic_deriv=stochastic_deriv/LA.norm(stochastic_deriv)*LA.norm(deriv)\n",
    "\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        new_point_x = starting_point[0] + stepsize*direction[0]\n",
    "        new_point_y = starting_point[1] + stepsize*direction[1]\n",
    "        new_point = np.array([new_point_x,new_point_y])\n",
    "\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*1.2\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*0.5\n",
    "        visited.append(starting_point)\n",
    "        count+=1\n",
    "    return {\"count\":count, \"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited)}\n",
    "\n",
    "# define the starting point\n",
    "starting_point = np.array([-0.5,1.5])\n",
    "# define the stepsize\n",
    "stepsize = 0.01\n",
    "# define the tolerance\n",
    "tol = 1e-5\n",
    "# call the function\n",
    "# set the derivative \n",
    "deriv = np.array([dfdx, dfdy])\n",
    "result = stochastic_gradient_descent(f,deriv,starting_point,stepsize,tol=tol,stochastic_injection=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578def2",
   "metadata": {},
   "source": [
    "#### (c) evaluate how much better or worse is the SGD convergence against the CG or BFGS method to find the global minimum, in terms of number of steps. Converge function/gradient to tolerance =1 Ã— 10-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "add66682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 20\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n",
      "     fun: 2.0711814827200667e-13\n",
      "     jac: array([ 4.94555024e-08, -2.45172016e-08])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 132\n",
      "     nit: 20\n",
      "    njev: 44\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.99999955, 0.99999908])\n"
     ]
    }
   ],
   "source": [
    "#CG methods \n",
    "res = minimize(f, starting_point, method='CG', options={'disp':True, 'gtol':1e-5})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fbe8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 22\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 31\n",
      "      fun: 1.6856836004019217e-13\n",
      " hess_inv: array([[0.50988602, 1.01962714],\n",
      "       [1.01962714, 2.08896666]])\n",
      "      jac: array([ 1.15312325e-07, -1.29424893e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 93\n",
      "      nit: 22\n",
      "     njev: 31\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.99999959, 0.99999917])\n"
     ]
    }
   ],
   "source": [
    "#CG methods \n",
    "res = minimize(f, starting_point, method='BFGS', options={'disp':True, 'gtol':1e-5})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d8e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182/3972538707.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (1-x)**2 + 10*(y-x**2)**2\n",
      "/tmp/ipykernel_182/3972538707.py:9: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return -2*(1-x) - 40*x*(y-x**2)\n",
      "/tmp/ipykernel_182/3972538707.py:14: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return 20*(y-x**2)\n",
      "/tmp/ipykernel_182/3972538707.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return (1-x)**2 + 10*(y-x**2)**2\n",
      "/tmp/ipykernel_182/3972538707.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -2*(1-x) - 40*x*(y-x**2)\n",
      "/tmp/ipykernel_182/3972538707.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 20*(y-x**2)\n",
      "/tmp/ipykernel_182/3972538707.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return (1-x)**2 + 10*(y-x**2)**2\n",
      "/tmp/ipykernel_182/3972538707.py:9: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return -2*(1-x) - 40*x*(y-x**2)\n",
      "/tmp/ipykernel_182/3972538707.py:14: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return 20*(y-x**2)\n",
      "/tmp/ipykernel_182/3972538707.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return (1-x)**2 + 10*(y-x**2)**2\n",
      "/tmp/ipykernel_182/3972538707.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -2*(1-x) - 40*x*(y-x**2)\n",
      "/tmp/ipykernel_182/3972538707.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 20*(y-x**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent:  1612.7\n",
      "Steepest Descent:  860.7\n",
      "Conjugate Gradient:  13.9\n",
      "BFGS:  14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182/3972538707.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_point_x = starting_point[0] + stepsize*direction[0]\n",
      "/tmp/ipykernel_182/3972538707.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  new_point_y = starting_point[1] + stepsize*direction[1]\n"
     ]
    }
   ],
   "source": [
    "def f(starting_point):\n",
    "    x = starting_point[0]\n",
    "    y = starting_point[1]\n",
    "    return (1-x)**2 + 10*(y-x**2)**2\n",
    "\n",
    "def dfdx(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return -2*(1-x) - 40*x*(y-x**2)\n",
    "\n",
    "def dfdy(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return 20*(y-x**2)\n",
    "\n",
    "def steepest_descent_count(func,first_derivate,starting_point,stepsize,tol=1e-5):\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "    deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "    deriv = np.array([deriv_x,deriv_y])\n",
    "    # initialize the counter\n",
    "    count=0\n",
    "    visited=[]\n",
    "    visited.append(starting_point)\n",
    "    while LA.norm(deriv) > tol and count < 1e6:\n",
    "        # calculate new point position\n",
    "        new_point_x = starting_point[0] - stepsize*deriv[0]\n",
    "        new_point_y = starting_point[1] - stepsize*deriv[1]\n",
    "        if func([new_point_x, new_point_y]) < func([starting_point[0], starting_point[1]]):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            stepsize = stepsize*1.2\n",
    "            visited.append(np.array([new_point_x,new_point_y]))\n",
    "            starting_point = np.array([new_point_x,new_point_y])\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            stepsize = stepsize*0.5\n",
    "            visited.append(np.array([new_point_x,new_point_y]))\n",
    "            starting_point = np.array([new_point_x,new_point_y])\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "        count+=1\n",
    "    # return the results\n",
    "    return count\n",
    "\n",
    "def stochastic_gradient_descent_count(func,first_derivate,starting_point,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    '''stochastic_injection: controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "        0 for no stochasticity, equivalent to SD. \n",
    "        Use 1 in this homework to run SGD\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "    deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "    deriv = np.array([deriv_x,deriv_y])\n",
    "    count=0\n",
    "    visited=[]\n",
    "    visited.append(starting_point)\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            # but has a random direction\n",
    "            stochastic_deriv=np.random.normal(0,1,2)\n",
    "            stochastic_deriv=stochastic_deriv/LA.norm(stochastic_deriv)*LA.norm(deriv)\n",
    "\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        new_point_x = starting_point[0] + stepsize*direction[0]\n",
    "        new_point_y = starting_point[1] + stepsize*direction[1]\n",
    "        new_point = np.array([new_point_x,new_point_y])\n",
    "\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*1.2\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*0.5\n",
    "        visited.append(starting_point)\n",
    "        count+=1\n",
    "    return count\n",
    "\n",
    "#run all the algorithms multiple times and different x and y positions and average the results \n",
    "#to get a better idea of the performance of the algorithms\n",
    "def run_multiple_times(func,first_derivate,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    #initialize the results\n",
    "    stochastic_result = []\n",
    "    steepest_result = []\n",
    "    cg_result = []\n",
    "    bfgs_result = []\n",
    "    #run the algorithm 10 times\n",
    "    for i in range(10):\n",
    "        #define a new starting point\n",
    "        starting_point = np.random.uniform(-1,1,2)\n",
    "        #run the algorithm\n",
    "        stochastic_res = stochastic_gradient_descent_count(func,first_derivate,starting_point,stepsize,tol=tol,stochastic_injection=stochastic_injection)\n",
    "        steepest_res = steepest_descent_count(func,first_derivate,starting_point,stepsize,tol=tol)\n",
    "        #only get iteration information from the algorithms\n",
    "        cg_res = minimize(func,starting_point,method='CG',options={'disp': False, 'gtol':tol}).nit\n",
    "        bfgs_res = minimize(func,starting_point,method='BFGS',options={'disp': False, 'gtol':tol}).nit\n",
    "        #append the result to the results\n",
    "        stochastic_result.append(stochastic_res)\n",
    "        steepest_result.append(steepest_res)\n",
    "        cg_result.append(cg_res)\n",
    "        bfgs_result.append(bfgs_res)\n",
    "    #find the averages of the results\n",
    "    stochastic = mean(stochastic_result)\n",
    "    steepest = mean(steepest_result)\n",
    "    cg = mean(cg_result)\n",
    "    bfgs = mean(bfgs_result)\n",
    "    return (stochastic ,steepest, cg, bfgs)\n",
    "\n",
    "stepsize = 0.01\n",
    "# define the tolerance\n",
    "tol = 1e-5\n",
    "# call the function\n",
    "# set the derivative \n",
    "deriv = np.array([dfdx, dfdy])\n",
    "#run the algorithm multiple times\n",
    "stochastic,steepest, cg, bfgs = run_multiple_times(f,deriv,stepsize,tol=tol,stochastic_injection=1)\n",
    "#print the results\n",
    "print(\"Stochastic Gradient Descent: \", stochastic)\n",
    "print(\"Steepest Descent: \", steepest)\n",
    "print(\"Conjugate Gradient: \", cg)\n",
    "print(\"BFGS: \", bfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c8320",
   "metadata": {},
   "source": [
    "# Question 4 \n",
    "\n",
    "### Stochastic  Gradient  Descent  with  Momentum  (SGDM).  The Rosenbrock Banana function with one minimum is not the best way to illustrate  the  power  of  the  SGD  or  SGDM  method.  Hence  we  next investigate the Three-Hump Camel function  ð‘“(ð‘¥,ð‘¦)=2ð‘¥2 âˆ’1.05ð‘¥4 +ð‘¥6 6â„ +ð‘¥ð‘¦+ð‘¦2 ð‘¥ âˆˆ[âˆ’2,2],ð‘¦âˆˆ[âˆ’2,2]  which is a convex function with three  minima. This defines our first â€œmultiple minimaâ€ problem where there is a global solution as well as two less optimal solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c3657",
   "metadata": {},
   "source": [
    "#### (a) Utilize SGD to find the global minimum, and compare it to CG or BFGS as you did in (2e). Starting from [-1.5,-1.5], converge function and gradient to tolerance = 1 Ã— 10-5 with stepsize =0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c6cf91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 21\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 8\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "stochastic Gradient Descent:\n",
      "{'x': array([-1.74755258,  0.87377426]), 'evaluation': 0.2986384422413273, 'path': array([[-1.5       , -1.5       ],\n",
      "       [-1.53669829, -1.45697603],\n",
      "       [-1.5490883 , -1.45277953],\n",
      "       [-1.47387109, -1.36561509],\n",
      "       [-1.45319337, -1.36644977],\n",
      "       [-1.50016442, -1.2177853 ],\n",
      "       [-1.54593221, -1.04209483],\n",
      "       [-1.51763925, -1.04259583],\n",
      "       [-1.38539769, -0.97478371],\n",
      "       [-1.30181522, -0.7016727 ],\n",
      "       [-1.16037569, -0.63315252],\n",
      "       [-1.18382473, -0.62309882],\n",
      "       [-1.22541745, -0.27991926],\n",
      "       [-1.10570087,  0.00423898],\n",
      "       [-1.18337462,  0.08691936],\n",
      "       [-1.09611816,  0.1198588 ],\n",
      "       [-1.1463142 ,  0.35184107],\n",
      "       [-1.13787242,  0.51286567],\n",
      "       [-1.20755716,  0.48088935],\n",
      "       [-1.27289655,  0.41723994],\n",
      "       [-1.6872839 ,  0.6951244 ],\n",
      "       [-1.65390877,  0.8302467 ],\n",
      "       [-1.80914778,  0.66795785],\n",
      "       [-1.74499123,  0.60739775],\n",
      "       [-1.6970376 ,  0.73175023],\n",
      "       [-1.70325169,  0.80494767],\n",
      "       [-1.71928371,  0.77392432],\n",
      "       [-1.70874077,  0.81943949],\n",
      "       [-1.73604717,  0.9022253 ],\n",
      "       [-1.81913019,  0.91852625],\n",
      "       [-1.80784365,  0.86032693],\n",
      "       [-1.80743497,  0.89388697],\n",
      "       [-1.73917646,  1.0342763 ],\n",
      "       [-1.80917828,  1.00213363],\n",
      "       [-1.80001849,  0.96523144],\n",
      "       [-1.77006558,  0.91812523],\n",
      "       [-1.74448739,  0.9305732 ],\n",
      "       [-1.73963522,  0.92200749],\n",
      "       [-1.73803793,  0.90230552],\n",
      "       [-1.77382611,  0.8843465 ],\n",
      "       [-1.75040412,  0.90576724],\n",
      "       [-1.74909618,  0.89648514],\n",
      "       [-1.74788466,  0.8886859 ],\n",
      "       [-1.74591366,  0.8868469 ],\n",
      "       [-1.74479381,  0.88476217],\n",
      "       [-1.757812  ,  0.88628762],\n",
      "       [-1.74041102,  0.8807369 ],\n",
      "       [-1.74364966,  0.88584144],\n",
      "       [-1.74526764,  0.88806984],\n",
      "       [-1.74409525,  0.88203134],\n",
      "       [-1.75865938,  0.87247654],\n",
      "       [-1.75132824,  0.86277115],\n",
      "       [-1.74706314,  0.8592564 ],\n",
      "       [-1.74303698,  0.86454478],\n",
      "       [-1.75283878,  0.86071125],\n",
      "       [-1.75176803,  0.8589741 ],\n",
      "       [-1.73966287,  0.8621357 ],\n",
      "       [-1.74677391,  0.8626089 ],\n",
      "       [-1.74690761,  0.86262783],\n",
      "       [-1.74782435,  0.86320986],\n",
      "       [-1.74851401,  0.86558249],\n",
      "       [-1.74887582,  0.86810714],\n",
      "       [-1.74388932,  0.86906629],\n",
      "       [-1.74521863,  0.86739564],\n",
      "       [-1.74785798,  0.86714303],\n",
      "       [-1.74665992,  0.86693815],\n",
      "       [-1.74644665,  0.86920484],\n",
      "       [-1.74818223,  0.87117317],\n",
      "       [-1.74645663,  0.87347228],\n",
      "       [-1.75041796,  0.87421985],\n",
      "       [-1.745061  ,  0.87344244],\n",
      "       [-1.74782726,  0.8761146 ],\n",
      "       [-1.74756604,  0.87511336],\n",
      "       [-1.74812149,  0.87468126],\n",
      "       [-1.74730555,  0.87446374],\n",
      "       [-1.74790632,  0.87447973],\n",
      "       [-1.74756904,  0.8740042 ],\n",
      "       [-1.74755228,  0.87400041],\n",
      "       [-1.74759041,  0.87395712],\n",
      "       [-1.74759409,  0.8739533 ],\n",
      "       [-1.74760473,  0.87392745],\n",
      "       [-1.74759173,  0.87385948],\n",
      "       [-1.74750143,  0.87381396],\n",
      "       [-1.74750139,  0.87379858],\n",
      "       [-1.74752264,  0.87375132],\n",
      "       [-1.74756098,  0.87378173],\n",
      "       [-1.74754393,  0.87377309],\n",
      "       [-1.7475446 ,  0.87376873],\n",
      "       [-1.74755731,  0.8737565 ],\n",
      "       [-1.74755543,  0.87375432],\n",
      "       [-1.74755038,  0.87375168],\n",
      "       [-1.74755   ,  0.87376206],\n",
      "       [-1.74755374,  0.87376202],\n",
      "       [-1.74755393,  0.87376224],\n",
      "       [-1.74754827,  0.8737664 ],\n",
      "       [-1.74755263,  0.8737719 ],\n",
      "       [-1.7475505 ,  0.87377342],\n",
      "       [-1.74755258,  0.87377426]])}\n",
      "CG:\n",
      "     fun: 0.2986384422397371\n",
      "     jac: array([8.40425491e-06, 7.45058060e-07])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 63\n",
      "     nit: 7\n",
      "    njev: 21\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-1.74755166,  0.87377619])\n",
      "BFGS:\n",
      "      fun: 0.2986384422368613\n",
      " hess_inv: array([[ 0.0856865 , -0.04290079],\n",
      "       [-0.04290079,  0.51091398]])\n",
      "      jac: array([ 5.96046448e-08, -2.98023224e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 30\n",
      "      nit: 8\n",
      "     njev: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-1.74755234,  0.87377615])\n"
     ]
    }
   ],
   "source": [
    "def f(starting_point):\n",
    "    x = starting_point[0]\n",
    "    y = starting_point[1]\n",
    "    return 2*x**2 - 1.05*x**4 + x**6/6 + x*y + y**2\n",
    "\n",
    "def dfdx(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return 4*x - 4.2*x**3 + x**5 + y\n",
    "\n",
    "def dfdy(points):\n",
    "    x = points[0]\n",
    "    y = points[1]\n",
    "    return x + 2*y\n",
    "\n",
    "def stochastic_gradient_descent(func,first_derivate,starting_point,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    '''stochastic_injection: controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "        0 for no stochasticity, equivalent to SD. \n",
    "        Use 1 in this homework to run SGD\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "    deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "    deriv = np.array([deriv_x,deriv_y])\n",
    "    count=0\n",
    "    visited=[]\n",
    "    visited.append(starting_point)\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            # but has a random direction\n",
    "            stochastic_deriv=np.random.normal(0,1,2)\n",
    "            stochastic_deriv=stochastic_deriv/LA.norm(stochastic_deriv)*LA.norm(deriv)\n",
    "\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        new_point_x = starting_point[0] + stepsize*direction[0]\n",
    "        new_point_y = starting_point[1] + stepsize*direction[1]\n",
    "        new_point = np.array([new_point_x,new_point_y])\n",
    "\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*1.2\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*0.5\n",
    "        visited.append(starting_point)\n",
    "        count+=1\n",
    "    return {\"count\":count, \"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited)}\n",
    "\n",
    "starting_point = np.array([-1.5,-1.5])\n",
    "stepsize = 0.01\n",
    "# define the tolerance\n",
    "tol = 1e-5\n",
    "# call the function\n",
    "# set the derivative \n",
    "deriv = np.array([dfdx, dfdy])\n",
    "#run the stochastic gradient descent\n",
    "stochastic = stochastic_gradient_descent(f, deriv, starting_point, stepsize, tol=tol, stochastic_injection=1)\n",
    "#run the CG \n",
    "cg = minimize(f, starting_point, method='CG', options={'disp':True, 'gtol':tol})\n",
    "bfgs = minimize(f, starting_point, method='BFGS', options={'disp':True, 'gtol':tol})\n",
    "print('stochastic Gradient Descent:')\n",
    "print(stochastic)\n",
    "print('CG:')\n",
    "print(cg)\n",
    "print('BFGS:')\n",
    "print(bfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e814e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent:  56.98\n",
      "Conjugate Gradient:  4.8\n",
      "BFGS:  5.74\n"
     ]
    }
   ],
   "source": [
    "#run all the algorithms multiple times and different x and y positions and average the results \n",
    "#to get a better idea of the performance of the algorithms\n",
    "def run_multiple_times(func,first_derivate,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    #initialize the results\n",
    "    stochastic_result = []\n",
    "    steepest_result = []\n",
    "    cg_result = []\n",
    "    bfgs_result = []\n",
    "    #run the algorithm 100 times\n",
    "    for i in range(100):\n",
    "        #define a new starting point\n",
    "        starting_point = np.random.uniform(-1,1,2)\n",
    "        #run the algorithm\n",
    "        stochastic_res = stochastic_gradient_descent_count(func,first_derivate,starting_point,stepsize,tol=tol,stochastic_injection=stochastic_injection)\n",
    "        #only get iteration information from the algorithms\n",
    "        cg_res = minimize(func,starting_point,method='CG',options={'disp': False, 'gtol':tol}).nit\n",
    "        bfgs_res = minimize(func,starting_point,method='BFGS',options={'disp': False, 'gtol':tol}).nit\n",
    "        #append the result to the results\n",
    "        stochastic_result.append(stochastic_res)\n",
    "        cg_result.append(cg_res)\n",
    "        bfgs_result.append(bfgs_res)\n",
    "    #find the averages of the results\n",
    "    stochastic = mean(stochastic_result)\n",
    "    cg = mean(cg_result)\n",
    "    bfgs = mean(bfgs_result)\n",
    "    return (stochastic, cg, bfgs)\n",
    "\n",
    "stepsize = 0.01\n",
    "# define the tolerance\n",
    "tol = 1e-5\n",
    "# call the function\n",
    "# set the derivative \n",
    "deriv = np.array([dfdx, dfdy])\n",
    "#run the algorithm multiple times\n",
    "stochastic,cg, bfgs = run_multiple_times(f,deriv,stepsize,tol=tol,stochastic_injection=1)\n",
    "#print the results\n",
    "print(\"Stochastic Gradient Descent: \", stochastic)\n",
    "print(\"Conjugate Gradient: \", cg)\n",
    "print(\"BFGS: \", bfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d63548",
   "metadata": {},
   "source": [
    "##### (b) Implement the SGDM algorithm with momentum ð›¾ =0.9. Now use SGD with Momentum to find the global minimum. Again start from [-1.5,-1.5] with stepsize = 0.1 and converge function and gradient to tolerance =1 Ã— 10-5. On average, did you get a better result using SGDM compared to SGD, CG or BFGS in finding the global minimum in terms of fewer steps? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5963aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'SGDM' took: 0.0073 sec\n",
      "{'count': 68, 'x': array([1.60773780e-06, 1.82006199e-06]), 'evaluation': 1.1408449770846599e-11, 'path': array([[-1.50000000e+00, -1.50000000e+00],\n",
      "       [-9.50666282e-02,  5.21168331e-01],\n",
      "       [-9.50666282e-02,  5.21168331e-01],\n",
      "       [ 1.09473127e-03,  3.93080327e-01],\n",
      "       [ 1.09473127e-03,  3.93080327e-01],\n",
      "       [-9.09227310e-02, -1.12724963e-01],\n",
      "       [-9.09227310e-02, -1.12724963e-01],\n",
      "       [-1.16276912e-01, -1.91166193e-02],\n",
      "       [-1.16276912e-01, -1.91166193e-02],\n",
      "       [-3.48882488e-02,  2.21755538e-03],\n",
      "       [-3.48882488e-02,  2.21755538e-03],\n",
      "       [-7.54208377e-03, -1.79247600e-03],\n",
      "       [-7.54208377e-03, -1.79247600e-03],\n",
      "       [-9.67001865e-03,  3.00113707e-03],\n",
      "       [-9.67001865e-03,  3.00113707e-03],\n",
      "       [ 3.08533970e-03, -8.24853984e-04],\n",
      "       [ 3.08533970e-03, -8.24853984e-04],\n",
      "       [ 5.05798960e-04, -4.52807604e-03],\n",
      "       [ 5.05798960e-04, -4.52807604e-03],\n",
      "       [-1.19661295e-03, -8.26811265e-04],\n",
      "       [-1.19661295e-03, -8.26811265e-04],\n",
      "       [ 2.61379324e-04,  3.00863773e-04],\n",
      "       [ 2.61379324e-04,  3.00863773e-04],\n",
      "       [-1.83079178e-04, -4.50353991e-05],\n",
      "       [-1.83079178e-04, -4.50353991e-05],\n",
      "       [-1.97665893e-05, -4.61575893e-06],\n",
      "       [-1.97665893e-05, -4.61575893e-06]])}\n"
     ]
    }
   ],
   "source": [
    "@timeit\n",
    "def SGDM(func,first_derivate,starting_point,stepsize,momentum=0.9,tol=1e-5,stochastic_injection=1):\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "    deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "    deriv = np.array([deriv_x,deriv_y])\n",
    "    count=0\n",
    "    visited=[]\n",
    "    visited.append(starting_point)\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            # but has a random direction\n",
    "            stochastic_deriv=np.random.normal(0,1,2)\n",
    "            stochastic_deriv=stochastic_deriv/LA.norm(stochastic_deriv)*LA.norm(deriv)\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        new_point_x = starting_point[0] + stepsize*direction[0]\n",
    "        new_point_y = starting_point[1] + stepsize*direction[1]\n",
    "        new_point = np.array([new_point_x,new_point_y])\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*1.2\n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*0.5\n",
    "            visited.append(starting_point)\n",
    "            # if stepsize is too small, clear previous direction because we already know that is not a useful direction\n",
    "            if stepsize<1e-5:\n",
    "                previous_direction=previous_direction-previous_direction\n",
    "            else:\n",
    "                # do the same as SGD here\n",
    "                previous_direction = new_point - starting_point\n",
    "            visited.append(starting_point)\n",
    "        count+=1\n",
    "    return {\"count\":count, \"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited)}\n",
    "\n",
    "# define the starting point\n",
    "starting_point = np.array([-1.5,-1.5])\n",
    "# define the stepsize\n",
    "stepsize = 0.1\n",
    "# define the tolerance\n",
    "tol = 1e-5\n",
    "# call the function\n",
    "# set the derivative \n",
    "deriv = np.array([dfdx, dfdy])\n",
    "# run the algorithm of stochastic gradient descent \n",
    "sgdm = SGDM(f,deriv,starting_point,stepsize,momentum=0.9,tol=tol,stochastic_injection=1)\n",
    "print(sgdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the algorithm multiple times to get the average time\n",
    "def SGDM_count(func,first_derivate,starting_point,stepsize,momentum=0.9,tol=1e-5,stochastic_injection=1):\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "    deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "    deriv = np.array([deriv_x,deriv_y])\n",
    "    count=0\n",
    "    visited=[]\n",
    "    visited.append(starting_point)\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            # but has a random direction\n",
    "            stochastic_deriv=np.random.normal(0,1,2)\n",
    "            stochastic_deriv=stochastic_deriv/LA.norm(stochastic_deriv)*LA.norm(deriv)\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        new_point_x = starting_point[0] + stepsize*direction[0]\n",
    "        new_point_y = starting_point[1] + stepsize*direction[1]\n",
    "        new_point = np.array([new_point_x,new_point_y])\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*1.2\n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            starting_point = new_point\n",
    "            deriv_x = first_derivate[0]([starting_point[0],starting_point[1]])\n",
    "            deriv_y = first_derivate[1]([starting_point[0],starting_point[1]])\n",
    "            deriv = np.array([deriv_x,deriv_y])\n",
    "            stepsize = stepsize*0.5\n",
    "            visited.append(starting_point)\n",
    "            # if stepsize is too small, clear previous direction because we already know that is not a useful direction\n",
    "            if stepsize<1e-5:\n",
    "                previous_direction=previous_direction-previous_direction\n",
    "            else:\n",
    "                # do the same as SGD here\n",
    "                previous_direction = new_point - starting_point\n",
    "            visited.append(starting_point)\n",
    "        count+=1\n",
    "    return count\n",
    "\n",
    "def run_multiple_times(func,first_derivate,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    #initialize the results\n",
    "    stochastic_result = []\n",
    "    steepest_result = []\n",
    "    cg_result = []\n",
    "    bfgs_result = []\n",
    "    sgdm_result = []\n",
    "    #run the algorithm 100 times\n",
    "    for i in range(10):\n",
    "        #define a new starting point\n",
    "        starting_point = np.random.uniform(-1,1,2)\n",
    "        #run the algorithm\n",
    "        sgdm_res = SGDM_count(func,first_derivate,starting_point,stepsize,momentum=0.9,tol=1e-5,stochastic_injection=1)\n",
    "        #append the result\n",
    "        sgdm_result.append(sgdm_res)\n",
    "    #find the averages of the results\n",
    "    sgdm = np.mean(sgdm_result)\n",
    "    return (sgdm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
