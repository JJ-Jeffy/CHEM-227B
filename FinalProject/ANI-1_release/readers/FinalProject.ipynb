{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f83474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from h5py) (1.24.2)\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d719dce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m372.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m893.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.11.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.2.0)\n",
      "Requirement already satisfied: wheel in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.1.tar.gz (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.1-py3-none-any.whl size=88172 sha256=3efbde0b9f98236466fbb904c9adf121da3ec0cb6f51304b6cebeeeac70db6b9\n",
      "  Stored in directory: /home/jeffy011/.cache/pip/wheels/33/c2/b7/b91592eb5167b4293827b97fb02d52686773dded13f7fb1054\n",
      "Successfully built lit\n",
      "Installing collected packages: mpmath, lit, cmake, typing-extensions, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.3 filelock-3.11.0 lit-16.0.1 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.11.1 torch-2.0.0 triton-2.0.0 typing-extensions-4.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c75f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchani\n",
      "  Downloading torchani-2.2.3-py3-none-any.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torchani) (2.0.0)\n",
      "Requirement already satisfied: requests in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torchani) (2.28.2)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-6.3.0-py3-none-any.whl (22 kB)\n",
      "Collecting lark-parser\n",
      "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from requests->torchani) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from requests->torchani) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from requests->torchani) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from requests->torchani) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (11.7.91)\n",
      "Requirement already satisfied: jinja2 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (3.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (11.7.101)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (11.7.99)\n",
      "Requirement already satisfied: networkx in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (3.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (10.9.0.58)\n",
      "Requirement already satisfied: typing-extensions in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from torch->torchani) (4.5.0)\n",
      "Requirement already satisfied: setuptools in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchani) (67.2.0)\n",
      "Requirement already satisfied: wheel in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchani) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from triton==2.0.0->torch->torchani) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from triton==2.0.0->torch->torchani) (16.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from jinja2->torch->torchani) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages (from sympy->torch->torchani) (1.3.0)\n",
      "Installing collected packages: lark-parser, zipp, importlib-metadata, torchani\n",
      "Successfully installed importlib-metadata-6.3.0 lark-parser-0.12.0 torchani-2.2.3 zipp-3.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13db8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m927.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.65.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c605a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyanitools as pya\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b523e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyanitools import anidataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b19fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages/torchani/aev.py:16: UserWarning: cuaev not installed\n",
      "  warnings.warn(\"cuaev not installed\")\n",
      "/home/jeffy011/CHEM277B/msse/lib/python3.10/site-packages/torchani/__init__.py:55: UserWarning: Dependency not satisfied, torchani.ase will not be available\n",
      "  warnings.warn(\"Dependency not satisfied, torchani.ase will not be available\")\n"
     ]
    }
   ],
   "source": [
    "import torchani\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26be8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7265f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ce875",
   "metadata": {},
   "source": [
    "### Understanding the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7401180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:    /gdb11_s01/gdb11_s01-0\n",
      "  Smiles:       [H]C([H])([H])[H]\n",
      "  Symbols:      ['C', 'H', 'H', 'H', 'H']\n",
      "  Coordinates:  (5400, 5, 3)\n",
      "  Energies:     (5400,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the HDF5 file containing the data\n",
    "data = anidataloader(\"../ani_gdb_s01.h5\")\n",
    "data_iter = data.__iter__()\n",
    "\n",
    "mols = next(data_iter)\n",
    "# Extract the data\n",
    "P = mols['path']\n",
    "X = mols['coordinates']\n",
    "E = mols['energies']\n",
    "S = mols['species']\n",
    "sm = mols['smiles']\n",
    "\n",
    "# Print the data\n",
    "print(\"Path:   \", P)\n",
    "print(\"  Smiles:      \",\"\".join(sm))\n",
    "print(\"  Symbols:     \", S)\n",
    "print(\"  Coordinates: \", X.shape)\n",
    "print(\"  Energies:    \", E.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd62f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of molecules: 3\n",
      "Total number of configurations: 10800\n",
      "Total number of unique species: 4\n",
      "{'H', 'N', 'O', 'C'}\n"
     ]
    }
   ],
   "source": [
    "data_iter = data.__iter__()\n",
    "count = 0 # count total number of molecules \n",
    "count_conf = 0 # count configuration \n",
    "species_set = set() # set to store unique species\n",
    "\n",
    "for mol in data_iter:\n",
    "    count += 1 \n",
    "    count_conf += len(mol['energies'])\n",
    "    species_set.update(set(mol['species'])) # add unique species to set\n",
    "\n",
    "print(\"Total number of molecules:\", count)\n",
    "print(\"Total number of configurations:\", count_conf)\n",
    "print(\"Total number of unique species:\", len(species_set))\n",
    "print(species_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54742395",
   "metadata": {},
   "source": [
    "As seen above, the dataset has several key features and mainly involves the smiles representation of the molecule showing how the atoms bond with each other. Additionally, the symbols of the atoms are also presented along with the total number of conformations (5400 in this case). In the coordinates variable, the number 5 presents the total number of atoms and the number 3 represents that there are three coordiantes for each atom. Further analysis shows that the file itself has 5 such molecules with over 10000 configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5bd2c",
   "metadata": {},
   "source": [
    "### Performing additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61120aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'H', 'H', 'H', 'H']\n",
      "[1 0 0 0 0]\n",
      "['C', 'H', 'H', 'H', 'H']\n",
      "[1 0 0 0 0]\n",
      "['C', 'H', 'H', 'H', 'H']\n",
      "[1 0 0 0 0]\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "Energies = []\n",
    "# Set the HDF5 file containing the data\n",
    "data = anidataloader(\"../ani_gdb_s01.h5\")\n",
    "data_iter = data.__iter__()\n",
    "\n",
    "for mol in data_iter: \n",
    "    # Extract the data\n",
    "    P = mols['path']\n",
    "    X = mols['coordinates']\n",
    "    E = mols['energies']\n",
    "    Energies.append(E)\n",
    "    S = mols['species']\n",
    "    sm = mols['smiles']\n",
    "\n",
    "    #constants used when defining AEV \n",
    "    Rcr = 5.2\n",
    "    EtaR = torch.tensor([16], dtype=torch.float)\n",
    "    ShfR = torch.tensor([0.900000,1.168750,1.437500,1.706250,1.975000,2.243750,2.51250,2.781250,3.050000,3.318750,3.587500,3.856250,4.125000,4.39375,4.662500,4.931250])\n",
    "    Rca = 3.5\n",
    "    EtaA = torch.tensor([8], dtype=torch.float)\n",
    "    ShfA = torch.tensor([0.900000,1.550000,2.200000,2.850000], dtype=torch.float)\n",
    "    ShfZ = torch.tensor([0.19634954,0.58904862,0.9817477,1.3744468,1.7671459,2.1598449,2.552544,2.945243]) \n",
    "    Zeta = torch.tensor([32], dtype=torch.float)\n",
    "    species_order = ['H', 'C', 'N', 'O']\n",
    "    num_species = len(species_order)\n",
    "\n",
    "    #should write this in the correct order to run \n",
    "    aev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA,Zeta, ShfA, ShfZ, num_species)\n",
    "\n",
    "    mapping = {\"H\":0, \"C\":1, \"N\":2, \"O\":3}\n",
    "    species = np.array([mapping[atom] for atom in S])\n",
    "    print(S)\n",
    "    print(species)\n",
    "    species = np.tile(species, (X.shape[0], 1))\n",
    "    species = torch.tensor(species)\n",
    "    X = torch.tensor(X)\n",
    "\n",
    "    aev_output = aev_computer((species, X)) #Species: (N,A) and coordinate:(N,A,3) and output: (N,A, #num_AEV)  \n",
    "    aev_output[1].shape\n",
    "\n",
    "    aev_output\n",
    "\n",
    "    output.append(aev_output)\n",
    "    \n",
    "print(aev_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893a846",
   "metadata": {},
   "source": [
    "Once the basics of the data set has been analyzed, much thought was given into represenation of the data set in order to create a system that accurately depicts the coformation of the atoms while also taking into account the various details such as transaltional, rotational and angular motions. This is done using the AEV calculator which acts as a representation of the atoms. This is done for each of the atoms in each of the molecules and saved for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b4a9a",
   "metadata": {},
   "source": [
    "### Creating an initial neural network \n",
    "\n",
    "This section is still under contruction. I have an idea of what I need to do and what I need to change but I have not been able to get a functioning code yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4c2f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print('func:%r  took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "    return wrap\n",
    "\n",
    "def create_chunks(complete_list, chunk_size=None, num_chunks=None):\n",
    "    '''\n",
    "    Cut a list into multiple chunks, each having chunk_size (the last chunk might be less than chunk_size) or having a total of num_chunk chunks\n",
    "    '''\n",
    "    chunks = []\n",
    "    if num_chunks is None:\n",
    "        num_chunks = math.ceil(len(complete_list) / chunk_size)\n",
    "    elif chunk_size is None:\n",
    "        chunk_size = math.ceil(len(complete_list) / num_chunks)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(complete_list[i * chunk_size: (i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, input_transform=lambda x: x):\n",
    "        \"\"\" The class for training the model\n",
    "        model: nn.Module\n",
    "            A pytorch model\n",
    "        optimizer_type: 'adam' or 'sgd'\n",
    "        learning_rate: float\n",
    "        epoch: int\n",
    "        batch_size: int\n",
    "        input_transform: func\n",
    "            transforming input. Can do reshape here\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if optimizer_type == \"sgd\":\n",
    "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            self.optimizer = Adam(model.parameters(), learning_rate)\n",
    "            \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "\n",
    "    @timing\n",
    "    def train(self, inputs, outputs, val_inputs, val_outputs,early_stop=False,l2=False,silent=False):\n",
    "        \"\"\" train self.model with specified arguments\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        val_nputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
    "        val_outputs: np.array shape (ndata,)\n",
    "        early_stop: bool\n",
    "        l2: bool\n",
    "        silent: bool. Controls whether or not to print the train and val error during training\n",
    "        \n",
    "        @return\n",
    "        a dictionary of arrays with train and val losses and accuracies\n",
    "        \"\"\"\n",
    "        inputs = self.input_transform(inputs)\n",
    "        val_inputs = self.input_transform(val_inputs)\n",
    "        ### convert data to tensor of correct shape and type here ###\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.int64)\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        \n",
    "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            batch_indices = list(range(inputs.shape[0]))\n",
    "            random.shuffle(batch_indices)\n",
    "            batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for batch in batch_indices:\n",
    "                batch_importance = len(batch) / len(outputs)\n",
    "                batch_input = inputs[batch]\n",
    "                batch_output = outputs[batch]\n",
    "                ### make prediction and compute loss with loss function of your choice on this batch ###\n",
    "                batch_predictions = self.model(batch_input)\n",
    "                #will need to adjust to account for a continous model \n",
    "#                 loss = nn.CrossEntropyLoss()(batch_predictions, batch_output)\n",
    "                if l2:\n",
    "                    ### Compute the loss with L2 regularization ###\n",
    "                    l2_lambda = 0.00001\n",
    "                    l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters())\n",
    "                    loss += l2_lambda * l2_norm\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                ### Compute epoch_loss and epoch_acc\n",
    "                epoch_loss += loss.detach().item() * batch_importance\n",
    "                pred = torch.argmax(batch_predictions, axis=-1)\n",
    "                #will need to adjust to account for continuous model\n",
    "#                 acc = torch.sum(pred == batch_output).item() / len(batch_predictions)\n",
    "                epoch_acc += acc * batch_importance\n",
    "            val_loss, val_acc = self.evaluate(val_inputs, val_outputs, print_acc=False)\n",
    "            if n_epoch % 10 ==0 and not silent: \n",
    "                print(\"Epoch %d/%d - Loss: %.3f - Acc: %.3f\" % (n_epoch + 1, self.epoch, epoch_loss, epoch_acc))\n",
    "                print(\"              Val_loss: %.3f - Val_acc: %.3f\" % (val_loss, val_acc))\n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "\n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)    \n",
    "\n",
    "        return {\"losses\": losses, \"accuracies\": accuracies, \"val_losses\": val_losses, \"val_accuracies\": val_accuracies}\n",
    "        \n",
    "    def evaluate(self, inputs, outputs, print_acc=True):\n",
    "        \"\"\" evaluate model on provided input and output\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        print_acc: bool\n",
    "        \n",
    "        @return\n",
    "        losses: float\n",
    "        acc: float\n",
    "        \"\"\"\n",
    "        inputs = self.input_transform(inputs)\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.int64)\n",
    "        self.model.eval()\n",
    "        batch_indices = list(range(inputs.shape[0]))\n",
    "        batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
    "        acc = 0\n",
    "        losses = 0\n",
    "        for batch in batch_indices:\n",
    "            batch_importance = len(batch) / len(outputs)\n",
    "            batch_input = inputs[batch]\n",
    "            batch_output = outputs[batch]\n",
    "            with torch.no_grad():\n",
    "                ### Compute prediction and loss###\n",
    "                batch_predictions = self.model(batch_input)\n",
    "                loss = nn.CrossEntropyLoss()(batch_predictions, batch_output)\n",
    "            pred = torch.argmax(batch_predictions, axis=-1)  \n",
    "            batch_acc = torch.sum(pred == batch_output) / len(batch_predictions)\n",
    "            losses += loss.detach().item() * batch_importance\n",
    "            acc += batch_acc.detach().item() * batch_importance\n",
    "        if print_acc:\n",
    "            print(\"Accuracy: %.3f\" % acc)\n",
    "        return losses, acc\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "training_data = aev_output[1]\n",
    "training_labels = Energies\n",
    "\n",
    "training_set = []\n",
    "validation_set = []\n",
    "training_label = []\n",
    "validation_label = []\n",
    "\n",
    "# Will have to further study the data set to understand how to split in such a way that the various indices can be taken into account \n",
    "# for train_index, val_index in kf.split(training_data):\n",
    "#     X_train, X_val = training_data[train_index], training_data[val_index]\n",
    "#     y_train, y_val = training_labels[train_index], training_labels[val_index]\n",
    "#     training_set.append(X_train)\n",
    "#     validation_set.append(X_val)\n",
    "#     training_label.append(y_train)\n",
    "#     validation_label.append(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253234d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1, padding=2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3*32*32, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 32, 32)\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "442d2387",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LeNet()\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, input_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x:x)\n\u001b[0;32m----> 7\u001b[0m run_info \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[43mtraining_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, training_label[i], validation_set[i], validation_label[i])\n\u001b[1;32m      8\u001b[0m run_info_list\u001b[38;5;241m.\u001b[39mappend(run_info)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, run_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracies\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# The training and validation accuracy of the model\n",
    "# The training and validation accuracy of the model\n",
    "run_info_list = []\n",
    "for i in range(3):\n",
    "    model = LeNet()\n",
    "    trainer = Trainer(model, optimizer_type='adam', learning_rate=1e-3, batch_size=128, epoch=30, input_transform = lambda x:x)\n",
    "    run_info = trainer.train(training_set[i], training_label[i], validation_set[i], validation_label[i])\n",
    "    run_info_list.append(run_info)\n",
    "    print(\"Training accuracy: \", run_info[\"accuracies\"][-1])\n",
    "    print(\"Validation accuracy: \", run_info[\"val_accuracies\"][-1])\n",
    "\n",
    "    # Plot the training and validation accuracy for fold 1 \n",
    "    plt.plot(run_info[\"accuracies\"], label=\"Training accuracy\")\n",
    "    plt.plot(run_info[\"val_accuracies\"], label=\"Validation accuracy\")\n",
    "    plt.title(\"Training and validation accuracy for fold %d\" % (i+1))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the training and validation loss for fold 1\n",
    "    plt.plot(run_info[\"losses\"], label=\"Training loss\")\n",
    "    plt.plot(run_info[\"val_losses\"], label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss for fold %d\" % (i+1))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Summarize the training and validation accuracy of the model over the three folds \n",
    "print(\"Summary: \")\n",
    "print(\"Training accuracy: \", np.mean([run_info[\"accuracies\"][-1] for run_info in run_info_list]))\n",
    "print(\"Validation accuracy: \", np.mean([run_info[\"val_accuracies\"][-1] for run_info in run_info_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188de59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
